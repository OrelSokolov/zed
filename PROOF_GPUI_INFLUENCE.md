# Как доказать, что GPUI влияет на TCP read

## Эксперименты для доказательства

### Эксперимент 1: Headless режим vs GUI режим

**Гипотеза**: Если проблема в GPUI/GUI, то в headless режиме задержки должны исчезнуть.

**Шаги**:
1. Запустить Zed в headless режиме: `ZED_HEADLESS=1 zed`
2. Запустить Ollama запрос и измерить время `read()`
3. Сравнить с GUI режимом

**Ожидаемый результат**:
- Headless: `read()` занимает 5-7мс (как в test_ollama.rs)
- GUI: `read()` занимает 40-45мс

**Если headless быстрый** → проблема в GUI компонентах (Wayland/X11 event loop, рендеринг, и т.д.)

---

### Эксперимент 2: Измерение времени пробуждения потока

**Гипотеза**: Поток пробуждается быстро, но выполнение откладывается из-за планировщика.

**Шаги**:
1. Добавить логирование времени пробуждения потока
2. Измерить время между пробуждением и фактическим `read()`

```rust
// В ollama.rs, перед read()
let wake_time = std::time::Instant::now();
let time_since_wake = wake_time.duration_since(last_wake_time);
eprintln!("[OLLAMA CONSOLE] Thread woke up, waited {}ms since last wake", time_since_wake.as_millis());

let read_start = std::time::Instant::now();
let n = tcp_stream.read(&mut read_buffer)?;
let read_time = read_start.elapsed();
```

**Ожидаемый результат**:
- Если `time_since_wake` большой (40-50мс) → планировщик откладывает пробуждение
- Если `time_since_wake` маленький, но `read_time` большой → проблема в самом `read()`

---

### Эксперимент 3: Perf trace с фильтром по TID

**Гипотеза**: Можно увидеть реальные системные вызовы и их задержки.

**Шаги**:
1. Запустить Zed и получить TID потока `ollama-stream-reader` из логов
2. Запустить `perf trace` с фильтром по TID:
```bash
# Получить TID из логов
TID=$(grep "Thread TID:" zed.log | tail -1 | awk '{print $NF}')

# Запустить perf trace
sudo perf trace -p $(pgrep -f zed) -T -e syscalls:sys_enter_read,syscalls:sys_exit_read --filter="tid == $TID"
```

**Ожидаемый результат**:
- Увидим реальные вызовы `read()` и время между `enter` и `exit`
- Если время большое → проблема в ядре/планировщике
- Если время маленькое, но задержка есть → проблема в пользовательском пространстве

---

### Эксперимент 4: Отключение worker threads

**Гипотеза**: Если проблема в конкуренции за CPU, уменьшение количества потоков должно помочь.

**Шаги**:
1. Модифицировать `crates/gpui/src/platform/linux/dispatcher.rs`:
```rust
// Вместо available_parallelism()
let thread_count = 1; // Минимум потоков
```

2. Пересобрать и протестировать

**Ожидаемый результат**:
- Если задержки уменьшились → проблема в конкуренции за CPU
- Если не изменилось → проблема не в количестве потоков

---

### Эксперимент 5: Измерение частоты обработки runnables

**Гипотеза**: Если главный поток обрабатывает много runnables, это может влиять на планировщик.

**Шаги**:
1. Логи уже есть: `[MAIN THREAD RUNNABLE] Processed X runnables in Ys, Z runnables/sec`
2. Сравнить время `read()` когда:
   - Много runnables обрабатывается (>100/sec)
   - Мало runnables (<10/sec)

**Ожидаемый результат**:
- Если задержки коррелируют с частотой runnables → проблема в главном потоке
- Если нет корреляции → проблема не в главном потоке

---

### Эксперимент 6: Изоляция потока через cgroups

**Гипотеза**: Если изолировать поток от других потоков Zed, задержки должны исчезнуть.

**Шаги**:
1. Создать cgroup для потока `ollama-stream-reader`
2. Назначить ему отдельный CPU
3. Измерить время `read()`

```bash
# Создать cgroup
sudo mkdir /sys/fs/cgroup/cpu/ollama_reader
echo 100000 > /sys/fs/cgroup/cpu/ollama_reader/cpu.cfs_quota_us
echo 100000 > /sys/fs/cgroup/cpu/ollama_reader/cpu.cfs_period_us

# Назначить поток (нужен PID потока, не процесса)
# Это сложно, нужно использовать pthread_setaffinity_np в коде
```

**Ожидаемый результат**:
- Если изоляция помогла → проблема в конкуренции за CPU
- Если не помогла → проблема не в CPU конкуренции

---

### Эксперимент 7: Сравнение с минимальным GPUI приложением

**Гипотеза**: Если создать минимальное GPUI приложение без всех компонентов Zed, можно изолировать проблему.

**Шаги**:
1. Создать минимальное GPUI приложение, которое только:
   - Запускает event loop
   - Создает отдельный поток для TCP read
   - Не делает ничего другого
2. Измерить время `read()`

**Ожидаемый результат**:
- Если минимальное приложение быстрое → проблема в компонентах Zed
- Если медленное → проблема в базовом GPUI

---

### Эксперимент 8: Мониторинг контекстных переключений

**Гипотеза**: Если поток часто переключается, это может объяснить задержки.

**Шаги**:
1. Использовать `perf stat` для подсчета контекстных переключений:
```bash
sudo perf stat -e context-switches,cpu-migrations -p $(pgrep -f zed) -I 1000
```

2. Сравнить с test_ollama.rs

**Ожидаемый результат**:
- Если много переключений → планировщик часто переключает потоки
- Если мало переключений → проблема не в переключениях

---

## Рекомендуемый порядок экспериментов

1. **Эксперимент 1** (Headless) - самый простой и быстрый
2. **Эксперимент 3** (Perf trace) - даст конкретные данные
3. **Эксперимент 2** (Время пробуждения) - покажет, где именно задержка
4. **Эксперимент 5** (Частота runnables) - проверит влияние главного потока
5. **Эксперимент 4** (Отключение worker threads) - проверит влияние фоновых потоков

---

## Критерии доказательства

**Если GPUI влияет**:
- ✅ Headless режим быстрее GUI
- ✅ Задержки коррелируют с частотой runnables
- ✅ Уменьшение worker threads уменьшает задержки
- ✅ Perf trace показывает задержки между пробуждением и выполнением

**Если планировщик ОС влияет**:
- ❌ Headless и GUI одинаково медленные
- ❌ Задержки не коррелируют с активностью GPUI
- ❌ Perf trace показывает, что `read()` сам по себе медленный
- ✅ Изоляция потока (cgroups/affinity) помогает

---

## Быстрый тест прямо сейчас

### Тест 1: Headless режим (самый простой)

**Способ: Запустить Zed в headless режиме**

```bash
# 1. Запустить Zed в headless режиме
ZED_HEADLESS=1 zed

# 2. В Zed запустить Ollama запрос через UI (Cmd+K или Ctrl+K, затем ввести запрос)
# 3. Смотреть логи [OLLAMA CONSOLE] Read ... bytes: total=Xms
#    Логи можно найти в: ~/.local/share/zed/logs/ или через stderr

# 4. Сравнить с обычным GUI режимом
zed  # без ZED_HEADLESS, затем повторить запрос
```

**Альтернатива: Использовать test_ollama.rs как базу**

Если headless режим Zed не работает, можно сравнить:
- `test_ollama.rs` (без GPUI) - должен быть быстрым (5-7мс)
- Zed в GUI режиме - должен быть медленным (40-45мс)

Если разница есть → проблема в GPUI.

**Ожидаемый результат**:
- Headless: `total=5-7ms` (быстро)
- GUI: `total=40-45ms` (медленно)

**Если headless быстрый** → проблема точно в GUI компонентах GPUI (Wayland/X11 event loop, рендеринг, и т.д.)

---

### Тест 2: Perf trace (самый точный)

```bash
# 1. Запустить Zed в обычном режиме
zed &

# 2. Получить PID и TID потока из логов
# В логах будет: [OLLAMA CONSOLE] Thread started (TID=12345)
TID=$(grep "Thread started (TID=" ~/.local/share/zed/logs/*.log | tail -1 | grep -oP 'TID=\K\d+')
PID=$(pgrep -f "zed$" | head -1)

# 3. Запустить perf trace с фильтром по TID
sudo perf trace -p $PID -T -e syscalls:sys_enter_read,syscalls:sys_exit_read --filter="tid == $TID" 2>&1 | tee perf_trace.log

# 4. В Zed запустить Ollama запрос
# 5. Остановить perf trace (Ctrl+C)
# 6. Анализировать perf_trace.log
```

**Что искать**:
- Время между `sys_enter_read` и `sys_exit_read` - это реальное время системного вызова
- Если время большое (40-45мс) → проблема в ядре/планировщике
- Если время маленькое (5-7мс), но в логах `total=40-45ms` → проблема в пользовательском пространстве (GPUI)

---

### Тест 3: Мониторинг активности главного потока

```bash
# 1. Запустить Zed
zed &

# 2. В логах уже есть: [MAIN THREAD RUNNABLE] Processed X runnables in Ys, Z runnables/sec
# 3. Запустить Ollama запрос и смотреть:
#    - Частоту runnables (Z runnables/sec)
#    - Время read() в [OLLAMA CONSOLE]

# 4. Сравнить:
#    - Когда много runnables (>100/sec) → время read()?
#    - Когда мало runnables (<10/sec) → время read()?
```

**Ожидаемый результат**:
- Если задержки коррелируют с частотой runnables → проблема в главном потоке GPUI
- Если нет корреляции → проблема не в главном потоке

---

### Тест 4: Сравнение с test_ollama.rs

```bash
# 1. Запустить test_ollama.rs (без Zed)
cd test_ollama
cargo run --release gpt-oss:20b 1000 "test"

# 2. Сравнить логи [RAW SOCKET] Read ... bytes: waited_since_last=X, read_time=Y
# 3. Запустить Zed и Ollama запрос
# 4. Сравнить логи [OLLAMA CONSOLE] Read ... bytes: total=Xms

# 5. Сравнить:
#    - test_ollama.rs: read_time=5-7ms
#    - Zed: total=40-45ms
```

**Ожидаемый результат**:
- test_ollama.rs быстрый → проблема в окружении Zed
- test_ollama.rs медленный → проблема в системе/сети

---

## Критерии доказательства

### Если GPUI влияет:
✅ Headless режим быстрее GUI (разница >30мс)  
✅ Задержки коррелируют с частотой runnables  
✅ Perf trace показывает задержки в пользовательском пространстве  
✅ test_ollama.rs быстрый, а Zed медленный  

### Если планировщик ОС влияет:
❌ Headless и GUI одинаково медленные  
❌ Задержки не коррелируют с активностью GPUI  
❌ Perf trace показывает, что `read()` сам по себе медленный (в ядре)  
✅ test_ollama.rs тоже медленный  

---

## Рекомендуемый порядок

1. **Тест 1 (Headless)** - 2 минуты, сразу покажет результат
2. **Тест 4 (test_ollama.rs)** - 1 минута, базовая проверка
3. **Тест 2 (Perf trace)** - 5 минут, точные данные
4. **Тест 3 (Runnables)** - уже есть в логах, просто анализ

Если headless быстрый → проблема точно в GUI компонентах GPUI.
